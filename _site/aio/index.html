















<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta http-equiv="last-modified" content="2022-02-10 18:19:54 -0500">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- meta "search-domain" used for google site search function google_search() -->
    <meta name="search-domain" value="">
    <link rel="stylesheet" type="text/css" href="../assets/css/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/lesson.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/syntax.css" />
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML'></script>


    



    <!-- Favicons for everyone -->
    <link rel="apple-touch-icon-precomposed" sizes="57x57" href="../assets/favicons/swc/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../assets/favicons/swc/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../assets/favicons/swc/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../assets/favicons/swc/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon-precomposed" sizes="60x60" href="../assets/favicons/swc/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon-precomposed" sizes="120x120" href="../assets/favicons/swc/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon-precomposed" sizes="76x76" href="../assets/favicons/swc/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../assets/favicons/swc/apple-touch-icon-152x152.png" />
    <link rel="icon" type="image/png" href="../assets/favicons/swc/favicon-196x196.png" sizes="196x196" />
    <link rel="icon" type="image/png" href="../assets/favicons/swc/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/png" href="../assets/favicons/swc/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="../assets/favicons/swc/favicon-16x16.png" sizes="16x16" />
    <link rel="icon" type="image/png" href="../assets/favicons/swc/favicon-128.png" sizes="128x128" />
    <meta name="application-name" content="Software Carpentry - Introduction to Machine Learning using Python"/>
    <meta name="msapplication-TileColor" content="#FFFFFF" />
    <meta name="msapplication-TileImage" content="../assets/favicons/swc/mstile-144x144.png" />
    <meta name="msapplication-square70x70logo" content="../assets/favicons/swc/mstile-70x70.png" />
    <meta name="msapplication-square150x150logo" content="../assets/favicons/swc/mstile-150x150.png" />
    <meta name="msapplication-wide310x150logo" content="../assets/favicons/swc/mstile-310x150.png" />
    <meta name="msapplication-square310x310logo" content="../assets/favicons/swc/mstile-310x310.png" />


    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
	<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
	<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
	<![endif]-->

  <title>
  Introduction to Machine Learning using Python
  </title>

  </head>
  <body>

    


<div class="panel panel-default life-cycle">
  <div id="life-cycle" class="panel-body beta">
    This lesson is being piloted (Beta version)
  </div>
</div>




    <div class="container">
      
















  
  










<nav class="navbar navbar-default">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      
      <a href="https://www.palmetto.clemson.edu" class="pull-left">
        <img class="navbar-logo" src="../assets/img/clemson.png" alt="West Chester University logo" />
      </a>

      
      <a class="navbar-brand" href="../index.html">Home</a>

    </div>
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">

        
	
        <li><a href="../setup.html">Setup</a></li>

        
        <li class="dropdown">
          <a href="../" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Episodes <span class="caret"></span></a>
          <ul class="dropdown-menu">
            
            
            <li><a href="../01-introduction/index.html">Introduction to Machine Learning</a></li>
            
            
            <li><a href="../02-sklearn-preprocessing/index.html">Introduction to Scikit Learn</a></li>
            
            
            <li><a href="../03-sklearn-Data-Partition/index.html">Data Partition with Scikit-Learn</a></li>
            
            
            <li><a href="../04.sklearn-Evaluation-Metrics/index.html">Evaluation Metrics with Scikit-Learn</a></li>
            
            
            <li><a href="../05-Training_Regression/index.html">Training Machine Learning model using Regression Method</a></li>
            
            
            <li><a href="../06-Training_Tree/index.html">Training Machine Learning model using Tree-based model</a></li>
            
            
            <li><a href="../07-Training_Ensemble/index.html">Training Machine Learning model using Ensemble approach</a></li>
            
            
            <li><a href="../08-Model_Based/index.html">Training Machine Learning model using Model based Prediction</a></li>
            
            
            <li><a href="../09-Regularization/index.html">Regularization and Variable Selection</a></li>
            
            
            <li><a href="../10-Dimension-Reduction/index.html">Dimension Reduction</a></li>
            
            
            <li><a href="../11-Neural-Network/index.html">Neural Network</a></li>
            
            
            <li><a href="../12-Support%20Vector%20Machine/index.html">Support Vector Machine</a></li>
            
            
            <li><a href="../13-KNN/index.html">K-Nearest Neighbour</a></li>
            
            
            <li><a href="../14-Unsupervised-Learning/index.html">Unsupervised Learning</a></li>
            
            
            <li><a href="../15-Mini-Project/index.html">Mini-Project</a></li>
            
	    <li role="separator" class="divider"></li>
            <li><a href="../aio/index.html">All in one page (Beta)</a></li>
          </ul>
        </li>
	

	
	
        <li class="dropdown">
          <a href="../" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Extras <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../reference.html">Reference</a></li>
            
            
            <li><a href="../Addon_DecisionTree/index.html">Detailed explanation on Decision Tree</a></li>
            
            
            <li><a href="../about/index.html">About</a></li>
            
            
            <li><a href="../figures/index.html">Figures</a></li>
            
            
            <li><a href="../guide/index.html">Instructor Notes</a></li>
            
          </ul>
        </li>
	

	
        <li><a href="../LICENSE.html">License</a></li>
      </ul>
      <form class="navbar-form navbar-right" role="search" id="search" onsubmit="google_search(); return false;">
        <div class="form-group">
          <input type="text" id="google-search" placeholder="Search..." aria-label="Google site search">
        </div>
      </form>
    </div>
  </div>
</nav>
















<h1 class="maintitle"><a href="../index.html">Introduction to Machine Learning using Python</a></h1>



<article>

<h1 id="introduction-to-machine-learning" class="maintitle">Introduction to Machine Learning</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 10 min
      <br />
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What is Machine Learning</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Learng basic about Machine Learning</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<p><img src="https://user-images.githubusercontent.com/43855029/114188431-25a80d00-9917-11eb-8262-25d658eba55e.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/43855029/114188470-2ccf1b00-9917-11eb-8f54-19bebb7b68a5.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/43855029/114188487-322c6580-9917-11eb-8bb8-746424eb2fbd.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/43855029/114188503-36588300-9917-11eb-8d12-18e884de7f2a.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/43855029/114188520-39537380-9917-11eb-8c12-5a6e06dfc1ac.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/43855029/114188556-42444500-9917-11eb-913f-66470b35450b.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/43855029/114188585-4a03e980-9917-11eb-9c6f-d3e499ca9ecc.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/43855029/114188613-512af780-9917-11eb-8d87-4f472023bc14.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/43855029/114188670-5e47e680-9917-11eb-989e-70cc709580f8.png" alt="image" /></p>

<ul>
  <li>Supervised Learning model that we are going to learn:</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114583677-da139d00-9c4f-11eb-816b-efee53facc2a.png" alt="image" /></p>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Basic Machine Learning</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="introduction-to-scikit-learn" class="maintitle">Introduction to Scikit Learn</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 40 min
      <br />
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What is Scikit Learn</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Master Scikit Learn for Machine Learning</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h2 id="21-what-is-scikit-learn">2.1 What is Scikit-Learn</h2>
<p><img src="https://user-images.githubusercontent.com/43855029/114609814-30db9f80-9c6d-11eb-8d4e-781f578e1d79.png" alt="image" /></p>

<ul>
  <li>Scikit-learn is probably the most useful library for machine learning in Python.</li>
  <li>The sklearn library contains a lot of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction.</li>
  <li>The sklearn package contains tools for:</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - data splitting
  - pre-processing
  - feature selection
  - model tuning using resampling
  - variable importance estimation
  as well as other functionality.
  
</code></pre></div></div>
<h2 id="22-install-sklearn">2.2 Install <code class="language-plaintext highlighter-rouge">sklearn</code></h2>
<p>We have installed the conda environment <code class="language-plaintext highlighter-rouge">skln</code> and include the scikit-learn package on Palmetto.</p>

<h2 id="23-pre-processing-using-sklearn">2.3 Pre-processing using <code class="language-plaintext highlighter-rouge">sklearn</code></h2>
<p>There are several steps that we will use <code class="language-plaintext highlighter-rouge">sklearn</code> for. For preprocessing raw data, we gonna use <code class="language-plaintext highlighter-rouge">sklearn</code> in these tasks:</p>
<ul>
  <li>Preprocessing with missing value</li>
  <li>Preprocessing: transform data</li>
</ul>

<h3 id="231-pre-processing-with-missing-value">2.3.1 Pre-processing with missing value</h3>
<ul>
  <li>Most of the time the input data has missing values (<code class="language-plaintext highlighter-rouge">NA, NaN, Inf</code>) due to data collection issue (power, sensor, personel).</li>
  <li>There are three main problems that missing data causes: missing data can introduce a substantial amount of bias, make the handling and analysis of the data more arduous, and create reductions in efficiency</li>
  <li>These missing values need to be treated/cleaned before we can use because “Garbage in =&gt; Garbage out”.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">data_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'/zfs/citi/workshop_data/python_ml/r_airquality.csv'</span><span class="p">))</span>
<span class="n">data_df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="../fig/02-preprocessing/01.png" style="height:300px" /></p>

<ul>
  <li>There are several ways to treat the missing values:</li>
  <li>Method 1: remove all missing <code class="language-plaintext highlighter-rouge">NA</code> values</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data1</span> <span class="o">=</span> <span class="n">data_df</span><span class="p">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">data1</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="../fig/02-preprocessing/02.png" style="height:300px" /></p>

<ul>
  <li>Method 2: Set <code class="language-plaintext highlighter-rouge">NA</code> to mean value</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data2</span> <span class="o">=</span> <span class="n">data_df</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">data2</span><span class="p">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">data2</span><span class="p">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="../fig/02-preprocessing/03.png" style="height:300px" /></p>

<ul>
  <li>Method 3: Use <code class="language-plaintext highlighter-rouge">Impute</code> to handle missing values</li>
</ul>

<p>In statistics, imputation is the process of replacing missing data with substituted values. 
Because missing data can create problems for analyzing data, imputation is seen as a way to 
avoid pitfalls involved with listwise deletion of cases that have missing values. That is to 
say, when one or more values are missing for a case, most statistical packages default to discarding 
any case that has a missing value, which may introduce bias or affect the representativeness of the 
results. Imputation preserves all cases by replacing missing data with an estimated value based on 
other available information. Once all missing values have been imputed, the data set can then be 
analysed using standard techniques for complete data. There have been many theories embraced by 
scientists to account for missing data but the majority of them introduce bias. A few of the well 
known attempts to deal with missing data include:</p>
<ul>
  <li>hot deck and cold deck imputation;</li>
  <li>listwise and pairwise deletion;</li>
  <li>mean imputation;</li>
  <li>non-negative matrix factorization;</li>
  <li>regression imputation;</li>
  <li>last observation carried forward;</li>
  <li>stochastic imputation;</li>
  <li>and multiple imputation.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="n">imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">missing_values</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s">'mean'</span><span class="p">)</span>
<span class="n">data3</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">imputer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data_df</span><span class="p">))</span>
<span class="n">data3</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">data_df</span><span class="p">.</span><span class="n">columns</span>
<span class="n">data3</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="../fig/02-preprocessing/04.png" style="height:320px" /></p>

<p><strong>Note:</strong>
SimpleImputer converts missing values to <strong>mean, median, most_frequent and constant</strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="n">imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">missing_values</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s">'median'</span><span class="p">)</span>
<span class="n">data4</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">imputer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data_df</span><span class="p">))</span>
<span class="n">data4</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">data_df</span><span class="p">.</span><span class="n">columns</span>
<span class="n">data4</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="../fig/02-preprocessing/05.png" style="height:320px" /></p>

<p><code class="language-plaintext highlighter-rouge">knnImpute</code> can also be used to fill in missing value</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">KNNImputer</span>
<span class="n">imputer</span> <span class="o">=</span> <span class="n">KNNImputer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s">"uniform"</span><span class="p">)</span>
<span class="n">data_knnimpute</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">imputer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data_df</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="../fig/02-preprocessing/06.png" style="height:300px" /></p>

<p><strong>Note:</strong></p>
<ul>
  <li>In addition to KNNImputer, there are <strong>IterativeImputer</strong> (Multivariate imputer that estimates each feature from all the others) and <strong>MissingIndicator</strong>(Binary indicators for missing values)</li>
  <li>More information on sklearn.impute can be found <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute">here</a></li>
</ul>

<h3 id="232-pre-processing-with-transforming-data">2.3.2 Pre-processing with Transforming data</h3>
<h4 id="2321-using-standardization">2.3.2.1 Using Standardization</h4>
<p><img src="https://user-images.githubusercontent.com/43855029/114231774-df6ba180-9948-11eb-9c61-3d2e0d3df889.png" alt="image" /></p>

<ul>
  <li>Standardization comes into picture when features of input data set have large differences between their ranges, or simply when they are measured in different measurement units for example: rainfall (0-1000mm), temperature (-10 to 40oC), humidity (0-100%), etc.</li>
  <li>Standardition Convert all independent variables into the same scale (mean=0, std=1)</li>
  <li>These differences in the ranges of initial features causes trouble to many machine learning models. For example, for the models that are based on distance computation, if one of the features has a broad range of values, the distance will be governed by this particular feature.</li>
  <li>The example below use data from above:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">scale</span>
<span class="n">data_std</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scale</span><span class="p">(</span><span class="n">data3</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">with_mean</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">with_std</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="n">data_std</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">data3</span><span class="p">.</span><span class="n">columns</span>
<span class="n">data_std</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
<span class="c1"># axis used to compute the means and standard deviations along. If 0, independently standardize each feature, otherwise (if 1) standardize each sample.
</span></code></pre></div>    </div>
  </li>
</ul>

<p><img src="../fig/02-preprocessing/07.png" style="height:300px" /></p>

<h4 id="2322-using-scaling-with-predefine-range">2.3.2.2 Using scaling with predefine range</h4>
<p>Transform features by scaling each feature to a given range.
This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.
Formulation for this is:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
X_scaled = X_std * (max - min) + min
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="c1">#By default, it scales for (0, 1) range
</span><span class="n">data_scaler</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaler</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data3</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="../fig/02-preprocessing/08.png" style="height:300px" /></p>

<h4 id="2323-using-box-cox-transformation">2.3.2.3 Using Box-Cox Transformation</h4>
<ul>
  <li>A <a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1964.tb00553.x">Box Cox</a> transformation is a transformation of a non-normal dependent variables into a normal shape.</li>
  <li>Normality is an important assumption for many statistical techniques; if your data isn’t normal, applying a Box-Cox means that you are able to run a broader number of tests.</li>
  <li>The Box Cox transformation is named after statisticians George Box and Sir David Roxbee Cox who collaborated on a 1964 paper and developed the technique.</li>
  <li>BoxCox can only be applied to stricly positive values
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">power_transform</span>
<span class="n">data_BxCx</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">power_transform</span><span class="p">(</span><span class="n">data3</span><span class="p">,</span><span class="n">method</span><span class="o">=</span><span class="s">"box-cox"</span><span class="p">))</span>
<span class="n">data_BxCx</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">data3</span><span class="p">.</span><span class="n">columns</span>
</code></pre></div>    </div>
  </li>
</ul>

<p><img src="../fig/02-preprocessing/09.png" style="height:300px" /></p>

<h4 id="2324-using-yeo-johnson-transformation">2.3.2.4 Using Yeo Johnson Transformation</h4>
<p>While BoxCox only works with positive value, a more recent transformation method <a href="https://www.jstor.org/stable/2673623">Yeo Johnson</a> can transform both positive and negative values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_yeo_johnson</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">power_transform</span><span class="p">(</span><span class="n">data3</span><span class="p">,</span><span class="n">method</span><span class="o">=</span><span class="s">"yeo-johnson"</span><span class="p">))</span>
<span class="n">data_yeo_johnson</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">data3</span><span class="p">.</span><span class="n">columns</span>
<span class="n">data_yeo_johnson</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="../fig/02-preprocessing/10.png" style="height:300px" /></p>

<p>Graphical representation of different transformation techniques</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">231</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data3</span><span class="p">[</span><span class="s">"Ozone"</span><span class="p">])</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Original probability"</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Ozone'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Count'</span><span class="p">)</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">232</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data_BxCx</span><span class="p">[</span><span class="s">"Ozone"</span><span class="p">])</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Box-Cox Transformation"</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Ozone'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Count'</span><span class="p">)</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">233</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data_yeo_johnson</span><span class="p">[</span><span class="s">"Ozone"</span><span class="p">])</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Yeo-Johnson Transformation"</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Ozone'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Count'</span><span class="p">)</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">234</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data_std</span><span class="p">[</span><span class="s">"Ozone"</span><span class="p">])</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Standard Transformation"</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Ozone'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Count'</span><span class="p">)</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">235</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data_scaler</span><span class="p">[</span><span class="s">"Ozone"</span><span class="p">])</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Scaling Transformation"</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Ozone'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Count'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="../fig/02-preprocessing/11.png" style="height:600px" /></p>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>sklearn</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="data-partition-with-scikit-learn" class="maintitle">Data Partition with Scikit-Learn</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 20 min
      <br />
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What is Data Partition</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Learn how to split data using sklearn</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h1 id="data-partition-training-and-testing">Data partition: training and testing</h1>

<p><img src="https://user-images.githubusercontent.com/43855029/120378647-b1716080-c2ec-11eb-8693-60defbbad7e2.png" alt="image" /></p>

<ul>
  <li>
    <p>In Machine Learning, it is mandatory to have training and testing set. Some time a verification set is also recommended.
Here are some functions for spliting training/testing set in <code class="language-plaintext highlighter-rouge">sklearn</code>:</p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">train_test_split</code>: create series of test/training partitions</li>
  <li><code class="language-plaintext highlighter-rouge">Kfold</code> splits the data into k groups</li>
  <li><code class="language-plaintext highlighter-rouge">StratifiedKFold</code> splits the data into k groups based on a grouping factor.</li>
  <li><code class="language-plaintext highlighter-rouge">RepeatKfold</code></li>
  <li><code class="language-plaintext highlighter-rouge">ShuffleSplit</code></li>
  <li><code class="language-plaintext highlighter-rouge">LeaveOneOut</code></li>
  <li><code class="language-plaintext highlighter-rouge">LeavePOut</code></li>
</ul>

<p>Due to time constraint, we only focus on <code class="language-plaintext highlighter-rouge">train_test_split</code>, <code class="language-plaintext highlighter-rouge">KFolds</code> and <code class="language-plaintext highlighter-rouge">StratifiedKFold</code></p>
<h2 id="31-scikit-learn-data">3.1 Scikit-Learn data</h2>
<p>The <code class="language-plaintext highlighter-rouge">sklearn.datasets</code> package embeds some small toy <a href="https://scikit-learn.org/stable/datasets.html">datasets</a></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>For each dataset, there are 4 varibles:
- **data**: numpy array of predictors/X
- **target**: numpy array of predictant/target/y
- **feature_names**: names of all predictors in X
- **target_names**: names of all predictand in y
</code></pre></div></div>
<p>For example:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">target_names</span><span class="p">)</span>
</code></pre></div></div>

<p>In this example we gonna use the renowned iris flower data</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">target</span>
</code></pre></div></div>

<h2 id="32-data-spliting-using-train_test_split-single-fold">3.2 Data spliting using <code class="language-plaintext highlighter-rouge">train_test_split</code>: <strong>Single fold</strong></h2>
<p>Here we use <code class="language-plaintext highlighter-rouge">train_test_split</code> to randomly split 60% data for training and the rest for testing:
<img src="https://user-images.githubusercontent.com/43855029/114209883-22b81700-992d-11eb-83a4-c4ab1538a1e5.png" alt="image" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="c1">#random_state: int, similar to R set_seed function
</span></code></pre></div></div>

<h2 id="33-data-spliting-using-k-fold">3.3 Data spliting using <code class="language-plaintext highlighter-rouge">K-fold</code></h2>
<ul>
  <li>This is the Cross-validation approach.</li>
  <li>This is a resampling process used to evaluate ML model on limited data sample.</li>
  <li>The general procedure:
    <ul>
      <li>Shuffle data randomly</li>
      <li>Split the data into <strong>k</strong> groups
  For each group:
        <ul>
          <li>Split into training &amp; testing set</li>
          <li>Fit a model on each group’s training &amp; testing set</li>
          <li>Retain the evaluation score and summarize the skill of model</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114211785-103edd00-992f-11eb-89d0-bbd7bd0c0178.png" alt="image" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="n">kf10</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">kf10</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">):</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_index</span><span class="p">]</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    
    <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1">#Training the model, not running now
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Accuracy for the fold no. </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s"> on the test set: </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="34-data-spliting-using-stratified-k-fold">3.4 Data spliting using <code class="language-plaintext highlighter-rouge">Stratified K-fold</code></h2>
<ul>
  <li>StratifiedKFold takes the cross validation one step further: it ensures that the target has balance class distribution.</li>
  <li>Look at the sample below:
The target has imbalanced class distribution with 12 values of 1 and 4 values of 0. KFold will not take that into consideration when splitting the Fold</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/120677513-2667a600-c465-11eb-814e-f4979ac9d123.png" alt="image" /></p>

<p>Here is the reuslt if using K-Fold:</p>

<p><img src="https://user-images.githubusercontent.com/43855029/120677884-8c542d80-c465-11eb-832a-bf05e1d73d28.png" alt="image" /></p>

<p>Here is the result of using Stratified K-Fold:</p>

<p><img src="https://user-images.githubusercontent.com/43855029/120677539-2d8eb400-c465-11eb-8227-9921b6f32362.png" alt="image" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="n">kf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>

<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">kf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">):</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_index</span><span class="p">]</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    
    <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1">#Training the model
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Accuracy for the fold no. </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s"> on the test set: </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>    
</code></pre></div></div>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>sklearn, data partition</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="evaluation-metrics-with-scikit-learn" class="maintitle">Evaluation Metrics with Scikit-Learn</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 20 min
      <br />
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>How do we measure the accuracy of ML model</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Learn different metrics with sklearn</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h1 id="4-evaluation-metrics">4 Evaluation Metrics</h1>

<ul>
  <li>Evaluation Metric is an essential part in any Machine Learning project.</li>
  <li>It measures how good or bad is your Machine Learning model</li>
  <li>Different Evaluation Metrics are used for Regression model (Continuous output) or Classification model (Categorical output).</li>
</ul>

<h2 id="41-regression-model-evaluation-metrics">4.1 Regression model Evaluation Metrics</h2>

<h3 id="411-correlation-coefficient-r-or-coefficient-of-determination-r2">4.1.1 Correlation Coefficient (R) or Coefficient of Determination (R2):</h3>

<p><img src="https://user-images.githubusercontent.com/43855029/120700259-72274900-c47f-11eb-8959-a4bbe4eafccc.png" alt="image" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="n">metrics</span><span class="p">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="412-root-mean-square-error-rmse-or-mean-square-error-mse">4.1.2 Root Mean Square Error (RMSE) or Mean Square Error (MSE)</h3>

<p><img src="https://user-images.githubusercontent.com/43855029/120700533-c5010080-c47f-11eb-8050-b1cd8c63746e.png" alt="image" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="n">metrics</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">squared</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># RMSE
</span><span class="n">metrics</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">squared</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># MSE
</span></code></pre></div></div>

<h2 id="42-classification-model-evaluation-metrics">4.2. Classification model Evaluation Metrics</h2>

<h3 id="421-confusion-matrix">4.2.1 Confusion Matrix</h3>
<ul>
  <li>A confusion matrix is a technique for summarizing the performance of a classification algorithm.</li>
  <li>You can learn more about Confusion Matrix <a href="https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/">here</a></li>
</ul>

<p>For binary output (classification problem with only 2 output type, also most popular):</p>

<p><img src="https://user-images.githubusercontent.com/43855029/120687356-efe35880-c46f-11eb-950f-5feef237a4c1.png" alt="image" /></p>

<h3 id="422-accuracy">4.2.2 Accuracy</h3>

<p>The most common metric for classification is accuracy, which is the fraction of samples predicted correctly as shown below:</p>

<p><img src="https://user-images.githubusercontent.com/43855029/120700619-dea24800-c47f-11eb-81c4-df090cad93da.png" alt="image" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="n">metrics</span><span class="p">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="423-precision">4.2.3 Precision</h3>

<p>Precision is the fraction of predicted positives events that are actually positive as shown below:</p>

<p><img src="https://user-images.githubusercontent.com/43855029/120700808-1c9f6c00-c480-11eb-9ec8-597d02a76a94.png" alt="image" /></p>

<h3 id="424-recall">4.2.4 Recall</h3>

<p>Recall (also known as sensitivity) is the fraction of positives events that you predicted correctly as shown below:</p>

<p><img src="https://user-images.githubusercontent.com/43855029/120700754-07c2d880-c480-11eb-81e1-7c7926452346.png" alt="image" /></p>

<h3 id="425-f1-score">4.2.5 F1 score</h3>

<p>The f1 score is the harmonic mean of recall and precision, with a higher score as a better model. The f1 score is calculated using the following formula:</p>

<p><img src="https://user-images.githubusercontent.com/43855029/120701061-6ee08d00-c480-11eb-9ab1-71d905e6a491.png" alt="image" /></p>

<p>More information on Precision, Recall and F1 score can be found <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html">here</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">metrics</span><span class="p">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">average</span><span class="o">=</span><span class="s">'binary'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="426-auc-roc-curve">4.2.6 AUC-ROC curve</h3>
<ul>
  <li>ROC: Receiver Operating Characteristics:  probability curve</li>
  <li>AUC: Area Under The Curve: represents the degree or measure of separability.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/120698991-ccbfa580-c47d-11eb-9f11-6e2acb00d46d.png" alt="image" /></p>

<ul>
  <li>AUC = 1:   perfect prediction</li>
  <li>AUC = 0.8: model has 80% chance to predict the right class</li>
  <li>AUC = 0.5: worst case, model has <strong>NO</strong> accuracy in prediction (random)</li>
  <li>AUC = 0:   the model is actually reciprocating the classes</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/120699552-84ed4e00-c47e-11eb-8089-54158439ad6f.png" alt="image" /></p>

<p>ROC Interpretation</p>

<p><img src="https://user-images.githubusercontent.com/43855029/133898061-2c7f5da6-c41b-41af-8a81-b65fef3c3184.png" alt="image" /></p>

<p>Code to calculate FPR, TPR:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
</code></pre></div></div>

<p>Code to calculate AUC score:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<span class="n">auc_score</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
</code></pre></div></div>

<p>We will go into detail how to plot AUC-ROC curve in the next chapter with a classification problem</p>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>sklearn, metrics</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="training-machine-learning-model-using-regression-method" class="maintitle">Training Machine Learning model using Regression Method</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 20 min
      <br />
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>How to train a Machine Learning model using Regression method</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Learn to use different Regression algorithm for Machine Learning training</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h1 id="5-supervised-learning-training-with-regression">5 Supervised Learning training with Regression</h1>
<h2 id="51-for-continuous-output">5.1 For continuous output</h2>
<h3 id="511-train-model-using-linear-regression-with-1-predictor">5.1.1 Train model using Linear Regression with 1 predictor</h3>
<p>Let use the <strong>airquality</strong> data in previous episodes:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">KNNImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="n">data_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/vuminhtue/Machine-Learning-Python/master/data/r_airquality.csv'</span><span class="p">))</span>

<span class="n">imputer</span> <span class="o">=</span> <span class="n">KNNImputer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s">"uniform"</span><span class="p">)</span>
<span class="n">data_knnimpute</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">imputer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data_df</span><span class="p">))</span>
<span class="n">data_knnimpute</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">data_df</span><span class="p">.</span><span class="n">columns</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data_knnimpute</span><span class="p">[</span><span class="s">'Temp'</span><span class="p">],</span>
                                                    <span class="n">data_knnimpute</span><span class="p">[</span><span class="s">'Ozone'</span><span class="p">],</span>
                                                    <span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</code></pre></div></div>
<p>Fit a Linear model using <code class="language-plaintext highlighter-rouge">method=lm</code></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">model_linreg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span><span class="bp">None</span><span class="p">],</span><span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>
<p>Apply trained model to testing data set and evaluate output using R-squared:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model_linreg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span><span class="bp">None</span><span class="p">])</span>
<span class="n">metrics</span><span class="p">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span> <span class="c1"># R^2
</span><span class="n">metrics</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">squared</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1">#RMSE
</span></code></pre></div></div>

<h3 id="512-train-model-using-multi-linear-regression-with-2-or-more-predictors">5.1.2 Train model using Multi-Linear Regression (with 2 or more predictors)</h3>
<p>From the above model, the <strong>R2=0.39</strong>:</p>

<p>The reason is that we only build the model with 1 input <code class="language-plaintext highlighter-rouge">Temp</code>.
In this section, we will build the model with more input <code class="language-plaintext highlighter-rouge">Solar Radiation, Wind, Temperature</code>:</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">,</span><span class="w"> </span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_train</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train_test_split</span><span class="p">(</span><span class="n">data_knnimpute</span><span class="p">[[</span><span class="s1">'Temp'</span><span class="p">,</span><span class="s1">'Wind'</span><span class="p">,</span><span class="s1">'Solar.R'</span><span class="p">]],</span><span class="w">
                                                    </span><span class="n">data_knnimpute</span><span class="p">[</span><span class="s1">'Ozone'</span><span class="p">],</span><span class="w">
                                                    </span><span class="n">train_size</span><span class="o">=</span><span class="m">0.6</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="m">123</span><span class="p">)</span><span class="w">
</span><span class="n">model_linreg</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">LinearRegression</span><span class="p">()</span><span class="n">.fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span><span class="w">
</span><span class="n">y_pred2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_linreg.predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="w">

</span><span class="n">metrics.r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred2</span><span class="p">)</span><span class="w">
</span><span class="n">metrics.mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred2</span><span class="p">,</span><span class="n">squared</span><span class="o">=</span><span class="n">False</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>
<p>Output is therefore better with smaller RMSE and higher Rsquared at <strong>0.5</strong></p>

<h3 id="513-train-model-using-polynomial-regression">5.1.3 Train model using Polynomial Regression</h3>
<p>From Multi-Linear Regression, the best <strong>R2=0.5</strong> using 3 predictors.
We can slightly improve this by using Polynomial Regression
<img src="https://user-images.githubusercontent.com/43855029/115059030-f7e13c00-9eb3-11eb-9887-52461d7a87aa.png" alt="image" /></p>

<p>In this study, let use polynomial regression with <code class="language-plaintext highlighter-rouge">degree of freedom=2</code></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_train_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data_knnimpute</span><span class="p">[[</span><span class="s">'Temp'</span><span class="p">,</span><span class="s">'Wind'</span><span class="p">,</span><span class="s">'Solar.R'</span><span class="p">]])</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_train_poly</span><span class="p">,</span>
                                                    <span class="n">data_knnimpute</span><span class="p">[</span><span class="s">'Ozone'</span><span class="p">],</span>
                                                    <span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">model_linreg_poly</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred_poly</span> <span class="o">=</span> <span class="n">model_linreg_poly</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred_poly</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred_poly</span><span class="p">,</span><span class="n">squared</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
</code></pre></div></div>
<p>The <strong>R2=0.58</strong> shows improvement using polynomial regression!</p>

<h2 id="52-for-categorical-output">5.2 For categorical output</h2>
<h3 id="521-train-model-using-logistic-regression">5.2.1 Train model using Logistic Regression</h3>
<ul>
  <li>Logistic regression is another technique borrowed by machine learning from the field of statistics. It is the go-to method for binary classification problems (problems with two class values).</li>
  <li>Typical binary classification: True/False, Yes/No, Pass/Fail, Spam/No Spam, Male/Female</li>
  <li>Unlike linear regression, the prediction for the output is transformed using a non-linear function called the logistic function.</li>
  <li>The standard logistic function has formulation:</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114233181-f7dcbb80-994a-11eb-9c89-58d7802d6b49.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/43855029/114233189-fb704280-994a-11eb-9019-8355f5337b37.png" alt="image" /></p>

<p>In this example, we create a sample data set and use logistic regression to solve it. The example is taken from <a href="https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/">here</a></p>

<p>Load library and create sample data set:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>

<span class="c1"># generate sample data
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Partitioning Data to train/test:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>Train model using Logistic Regression</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">model_LogReg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model_LogReg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">model_LogReg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># predict output:
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model_LogReg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># predict probabilities
</span><span class="n">lr_probs</span> <span class="o">=</span> <span class="n">model_LogReg</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<p>Evaluate output with accurary level:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="n">metrics</span><span class="p">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
</code></pre></div></div>
<p>We retrieve the <strong>accuracy = 0.834</strong></p>

<p>Now compute AUC-ROC and plot curve</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># generate a no skill prediction (majority class)
</span><span class="n">ns_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">))</span>

<span class="c1"># calculate scores
</span><span class="n">ns_auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">ns_probs</span><span class="p">)</span>
<span class="n">lr_auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_probs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="c1"># summarize scores
</span><span class="k">print</span><span class="p">(</span><span class="s">'No Skill: ROC AUC=%.3f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">ns_auc</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Logistic: ROC AUC=%.3f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">lr_auc</span><span class="p">))</span>
<span class="c1"># calculate roc curves
</span><span class="n">ns_fpr</span><span class="p">,</span> <span class="n">ns_tpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">ns_probs</span><span class="p">)</span>
<span class="n">lr_fpr</span><span class="p">,</span> <span class="n">lr_tpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_probs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="c1"># plot the roc curve for the model
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ns_fpr</span><span class="p">,</span> <span class="n">ns_tpr</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'No Skill'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lr_fpr</span><span class="p">,</span> <span class="n">lr_tpr</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'.'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Logistic'</span><span class="p">)</span>
<span class="c1"># axis labels
</span><span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'False Positive Rate'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True Positive Rate'</span><span class="p">)</span>
<span class="c1"># show the legend
</span><span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># show the plot
</span><span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/43855029/120822169-22e72400-c524-11eb-97fe-46f711a11072.png" alt="image" /></p>

<p>An alternative way to plot AUC-ROC curve, using additional toolbox <a href="https://scikit-plot.readthedocs.io/en/stable/">“scikit-plot”</a></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pip</span> <span class="n">install</span> <span class="n">scikit</span><span class="o">-</span><span class="n">plot</span>
</code></pre></div></div>

<p>The shorter code for using this library:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scikitplot</span> <span class="k">as</span> <span class="n">skplt</span>
<span class="n">skplt</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="n">plot_roc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_probs</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/43855029/120822378-588c0d00-c524-11eb-9cdc-431bd927ad48.png" alt="image" /></p>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Regression training</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="training-machine-learning-model-using-tree-based-model" class="maintitle">Training Machine Learning model using Tree-based model</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 20 min
      <br />
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>How to train a Machine Learning model using Tree-based model</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Learn to use different Tree-based algorithm for Machine Learning training</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h1 id="supervised-learning-training">Supervised Learning training</h1>

<h2 id="train-model-using-decision-tree">Train model using Decision Tree</h2>
<ul>
  <li>Tree based learning algorithms are considered to be one of the best and mostly used supervised learning methods.</li>
  <li>Tree based methods empower predictive models with high accuracy, stability and ease of interpretation</li>
  <li>Non-parametric and non-linear relationships</li>
  <li>Types: Categorical and Continuous
<img src="https://user-images.githubusercontent.com/43855029/114233972-198a7280-994c-11eb-9f4f-da4ed958961e.png" alt="image" /></li>
</ul>

<h3 id="spliting-algorithm">Spliting algorithm</h3>
<ul>
  <li>Gini Impurity: (Categorical)</li>
  <li>Chi-Square index (Categorical)</li>
  <li>Cross-Entropy &amp; Information gain (Categorical)</li>
  <li>Reduction Variance (Continuous)</li>
</ul>

<p>More information on how to apply the spliting algorithm to split the data can be found <a href="https://clemsonciti.github.io/Workshop-Python-ML/Addon_DecisionTree/index.html">here</a></p>

<h3 id="pros--cons">Pros &amp; Cons</h3>
<p><img src="https://user-images.githubusercontent.com/43855029/114234120-548ca600-994c-11eb-889e-e8ec6d313e52.png" alt="image" /></p>

<h3 id="implementation">Implementation</h3>
<p>Here we will use <code class="language-plaintext highlighter-rouge">iris</code> data</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</code></pre></div></div>
<p>Next we will train using <code class="language-plaintext highlighter-rouge">DecisionTree</code> with <code class="language-plaintext highlighter-rouge">gini</code> splitting algorithm:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="n">model_DT</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">criterion</span><span class="o">=</span><span class="s">"gini"</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>
<p>Once done, we can visualize the tree:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="n">tree</span><span class="p">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">model_DT</span><span class="p">)</span>
</code></pre></div></div>
<p>However, in order to have a nicer plot:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">graphviz</span>
<span class="n">dot_data</span> <span class="o">=</span> <span class="n">tree</span><span class="p">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">model_DT</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>                      
                      <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">feature_names</span><span class="o">=</span><span class="n">iris</span><span class="p">.</span><span class="n">feature_names</span><span class="p">,</span>
                      <span class="n">special_characters</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  
<span class="n">graph</span> <span class="o">=</span> <span class="n">graphviz</span><span class="p">.</span><span class="n">Source</span><span class="p">(</span><span class="n">dot_data</span><span class="p">)</span> 
<span class="n">graph</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/43855029/134966642-8f3d3009-3d3d-494b-890b-c1295bd01970.png" alt="image" /></p>

<p>Apply decision tree model to predic output of testing data</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="n">y_pred_DT</span> <span class="o">=</span> <span class="n">model_DT</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">metrics</span><span class="p">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred_DT</span><span class="p">)</span>
</code></pre></div></div>
<p>The <strong>accuracy=0.95</strong></p>

<p>More information on Decision Tree can be found <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">here</a></p>

<h2 id="train-model-using-random-forest">Train model using Random Forest</h2>
<p><img src="https://user-images.githubusercontent.com/43855029/115076000-f3278280-9ec9-11eb-89b4-b07f3713b105.png" alt="image" /></p>

<ul>
  <li>Random Forest is considered to be a panacea of all data science problems. On a funny note, when you can’t think of any algorithm (irrespective of situation), use random forest!</li>
  <li>Opposite to Decision Tree, Random Forest use bootstrapping technique to grow multiple tree</li>
  <li>Random Forest is a versatile machine learning method capable of performing both regression and classification tasks.</li>
  <li>It is a type of ensemble learning method, where a group of weak models combine to form a powerful model.</li>
  <li>The end output of the model is like a black box and hence should be used judiciously.
    <h3 id="detail-explaination">Detail explaination</h3>
  </li>
  <li>If there are M input variables, a number m&lt;M is specified such that at each node, m variables are selected at random out of the M. The best split on these m is used to split the node. The value of m is held constant while we grow the forest.</li>
  <li>Each tree is grown to the largest extent possible and  there is no pruning.</li>
  <li>Predict new data by aggregating the predictions of the ntree trees (i.e., majority votes for classification, average for regression).</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114235192-d16c4f80-994d-11eb-9732-571463c2f3f5.png" alt="image" /></p>

<h3 id="pros--cons-of-random-forest">Pros &amp; Cons of Random Forest</h3>
<p><img src="https://user-images.githubusercontent.com/43855029/114235213-daf5b780-994d-11eb-83f8-ac7520749dbe.png" alt="image" /></p>

<h3 id="implementation-of-random-forest">Implementation of Random Forest</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="n">model_RF</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span><span class="n">criterion</span><span class="o">=</span><span class="s">"gini"</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred_RF</span> <span class="o">=</span> <span class="n">model_RF</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">metrics</span><span class="p">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred_RF</span><span class="p">)</span>
</code></pre></div></div>
<p>The <strong>accuracy=0.97</strong></p>

<p>In this example, we use <code class="language-plaintext highlighter-rouge">n_estimators=20</code> to grow <code class="language-plaintext highlighter-rouge">n</code> number of trees in the forest.
We can see that Random Forest result has better prediction than Decision Tree.</p>

<p>More information on Random Forest can be found <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random#sklearn.ensemble.RandomForestClassifier">here</a></p>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Decision Tree, Random Forest</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="training-machine-learning-model-using-ensemble-approach" class="maintitle">Training Machine Learning model using Ensemble approach</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 20 min
      <br />
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>How to overcome limitation of single ML model?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Learn to use different Ensemble ML algorithm for Machine Learning training</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h2 id="71-why-ensemble">7.1 Why Ensemble:</h2>
<p>Ensemble is a method in Machine Learning that <strong>combine decision from several ML models</strong> to obtain optimum output.
This espisode get information from <a href="https://www.pluralsight.com/guides/ensemble-methods:-bagging-versus-boosting">here</a></p>

<p><img src="https://user-images.githubusercontent.com/43855029/115078334-7b5b5700-9ecd-11eb-93fb-c3f69e740a5c.png" alt="image" />
<a href="https://www.patheos.com/blogs/driventoabstraction/2018/07/blind-men-elephant-folklore-knowledge/">Source: Patheos.com</a></p>

<p>Ensemble approaches can reduce variance &amp; Avoid Overfitting by combining results of multiple classifiers on different sub-samples</p>

<p><img src="https://user-images.githubusercontent.com/43855029/114235479-417ad580-994e-11eb-806b-2f73996f864d.png" alt="image" /></p>

<p>Figure. Bias &amp; Variance Tradeoff</p>

<ul>
  <li>Bias: the difference between the model prediction &amp; observation. High bias: model did not train well.</li>
  <li>Variance: the variability of model prediction from one point to another. High variance: model performs really well in training but having high error rate in testing set</li>
</ul>

<h2 id="72-train-model-using-ensemble-approach">7.2 Train model using Ensemble Approach</h2>
<p>Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.
Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.
Here we will be learning several ensemble models:</p>
<ul>
  <li>Random Forest</li>
  <li>Bagging</li>
  <li>Boosting</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/115079289-f6713d00-9ece-11eb-90cb-7084e8d7a536.png" alt="image" /></p>

<h2 id="73-train-model-using-bagging-bootstrap-aggregation">7.3 Train model using Bagging (Bootstrap Aggregation)</h2>
<ul>
  <li>The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.</li>
  <li>Bootstrap randomly create a small subsets of data from entire dataset</li>
  <li>The subset data has similar characteristic as the entire dataset.</li>
</ul>

<h3 id="731-detail-explaination-of-bagging">7.3.1 Detail explaination of Bagging</h3>
<p>There are 3 steps in Bagging</p>

<p><img src="https://user-images.githubusercontent.com/43855029/115079407-202a6400-9ecf-11eb-9c9c-7f3a0bbf1c28.png" alt="image" /></p>

<p>Step 1: Here you replace the original data with new sub-sample data using bootstrapping.</p>

<p>Step 2: Train each sub-sample data using ML algorithm</p>

<p>Step 3: Lastly, you use an average value to combine the predictions of all the classifiers, depending on the problem. Generally, these combined values are more robust than a single model.</p>

<h3 id="732-implementation-of-bagging">7.3.2 Implementation of Bagging</h3>
<p>Here we use iris data set:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">123</span><span class="p">)</span>
</code></pre></div></div>
<p>First apply <strong>Bagging</strong> with <strong>DecisionTree</strong> model, Bagging’s parameter can be found <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html">here</a>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="n">model_DT</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>

<span class="n">model_bag_DT</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">model_DT</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                            <span class="n">bootstrap</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">model_bag_DT</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">model_bag_DT</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">),</span><span class="n">model_bag_DT</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>
<p>The output accuracy from <strong>Bagging</strong> with <strong>DecisionTree</strong> for train/testing have : <code class="language-plaintext highlighter-rouge">(1.0, 0.9666666666666667)</code></p>

<h2 id="74-train-model-using-boosting">7.4 Train model using Boosting</h2>

<p>http://uc-r.github.io/public/images/analytics/gbm/boosted_stumps.gif</p>

<ul>
  <li>Boosting is an approach to convert weak predictors to get stronger predictors.</li>
  <li>Boosting follows a sequential order: output of base learner will be input to another</li>
  <li>If a base classifier is misclassifier (red box), its weight is increased and the next base learner will classify more correctly.</li>
  <li>Finally combine the classifier to predict result</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/115079476-39331500-9ecf-11eb-9af5-cb3cb2948cf0.png" alt="image" /></p>

<p>More information on Boosting can be found <a href="https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/">here</a></p>

<h3 id="741-adaptive-boosting-adaboost">7.4.1 Adaptive Boosting: Adaboost</h3>
<ul>
  <li>Adaptive: weaker learners are tweaked by misclassify from previous classifier</li>
  <li>AdaBoost is best used to boost the performance of decision trees on binary classification problems.</li>
  <li>Better for classification rather than regression.</li>
  <li>Sensitive to noise</li>
</ul>

<h4 id="implementation-of-adaboost">Implementation of Adaboost</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<span class="n">model_AD</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.03</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">model_AD</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">),</span><span class="n">model_AD</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>
<p>The output accuracy from <strong>AdaBoost</strong>  for train/testing have : <code class="language-plaintext highlighter-rouge">(0.9333333333333333, 0.8333333333333334)</code></p>

<h3 id="742-gradient-boosting-machines">7.4.2 Gradient Boosting Machines:</h3>
<ul>
  <li>Extremely popular ML algorithm</li>
  <li>Widely used in Kaggle competition</li>
  <li>Ensemble of shallow and weak successive tree, with each tree learning and improving on the previous</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="n">model_GBM</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">model_GBM</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">),</span><span class="n">model_GBM</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>
<p>The output accuracy from <strong>GradientBoosting</strong>  for train/testing have : <code class="language-plaintext highlighter-rouge">(1.0, 0.9333333333333333)</code></p>

<h2 id="75-compare-bagging-and-boosting-technique">7.5 Compare Bagging and Boosting technique:</h2>
<p><img src="https://user-images.githubusercontent.com/43855029/115079914-e443ce80-9ecf-11eb-8b19-622abbfe026c.png" alt="image" /></p>

<h2 id="76-conclusions">7.6 Conclusions</h2>
<ul>
  <li>Ensemble overcome the limitation of using only single model</li>
  <li>Between bagging and boosting, there is no better approach without trial &amp; error.</li>
</ul>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Bagging, Boosting</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="training-machine-learning-model-using-model-based-prediction" class="maintitle">Training Machine Learning model using Model based Prediction</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 20 min
      <br />
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What is model based prediction algorithm in ML?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Learn to use different Model based prediction for Machine Learning training</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h2 id="81-naive-bayes">8.1 Naive Bayes</h2>
<ul>
  <li>Assuming data follow a probabilistic model</li>
  <li>Assuming all predictors are independent (Naïve assumption)</li>
  <li>Use Bayes’s theorem to identify optimal classifiers</li>
  <li>More information on Aplication of Bayes’s Theorem in ML can be found <a href="https://machinelearningmastery.com/bayes-theorem-for-machine-learning/">here</a></li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114339414-20b7a900-9b23-11eb-9ae1-39640f50e06c.png" alt="image" />
<img src="https://user-images.githubusercontent.com/43855029/114339497-62485400-9b23-11eb-8511-29e1c9077946.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/43855029/114339516-6f654300-9b23-11eb-838c-aaf600ca922a.png" alt="image" /></p>

<h3 id="811-implementation-naive-bayes">8.1.1 Implementation Naive Bayes</h3>
<p>Split data</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">123</span><span class="p">)</span>
</code></pre></div></div>

<p>Train data using Naive Bayes</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="n">model_NB</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">model_NB</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">model_NB</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>
<p>In addition to <strong>GaussianNB</strong>, sklearn also includes: <strong>MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB</strong>.
More information on Naive Bayes using sklearn can be found <a href="https://scikit-learn.org/stable/modules/naive_bayes.html">here</a></p>

<h2 id="82-linear-discriminent-analysis">8.2 Linear Discriminent Analysis</h2>
<ul>
  <li>LDA is a supervised learning model that is similar to logistic regression in that the outcome variable is categorical and can therefore be used for classification.</li>
  <li>LDA is useful with two or more class of objects</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114339862-3bd6e880-9b24-11eb-9f4f-8f3af989c724.png" alt="image" /></p>

<h3 id="821-implementation-lda">8.2.1 Implementation LDA</h3>
<p>Using the same iris data set, the LDA model is built:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="n">model_LDA</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">model_LDA</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">model_LDA</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="822-ensemble-approach-bagging-with-lda">8.2.2 Ensemble approach (Bagging) with LDA</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="n">model_LDAbag</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">base_estimator</span> <span class="o">=</span> <span class="n">model_LDA</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                 <span class="n">bootstrap</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                                 <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">model_LDAbag</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">model_LDAbag</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">model_LDAbag</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Naive Bayes, Linear Discriminent Analyst</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="regularization-and-variable-selection" class="maintitle">Regularization and Variable Selection</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 20 min
      <br />
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>Why do we need Regularization and Variable Selection in ML model</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Learn how to apply Regularization and Variable selection in ML model</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<p><img src="https://user-images.githubusercontent.com/43855029/114340188-ff57bc80-9b24-11eb-826a-69cb444687d4.png" alt="image" /></p>
<ul>
  <li>One of the major aspects of training your machine learning model is to avoid overfitting (Using more parameter to best fit the training but on the other hand, failed to evaluate the testing).</li>
  <li>The concept of balancing bias and variance, is helpful in understanding the phenomenon of overfitting</li>
</ul>

<h2 id="9-regularization">9 Regularization</h2>
<ul>
  <li>In order to reduce the Model Complexity or to avoid Multi-Collinearity, one needs to reduce the number of covariates 
(or set the coefficient to be zero).</li>
  <li>If the coefficients are too large, let’s penalize them to enforce them to be smaller</li>
  <li>Regularization is a form of multilinear regression, that constrains/regularizes or shrinks the coefficient estimates towards zero.</li>
  <li>In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting</li>
  <li>A simple Multi-Linear Regression look like this:
<img src="https://user-images.githubusercontent.com/43855029/114416230-766d6f00-9b7e-11eb-800b-2b7a65782859.png" alt="image" /></li>
</ul>

<p>=&gt; in which: <strong>β</strong> represents the coefficient estimates for different variables or predictors(x)</p>

<p>The residual sum of squares <strong>RSS</strong> is the loss function of the fitting procedure.
And we need to determine the optimal coefficients <strong>𝛽</strong> to minimize the loss function</p>

<p><img src="https://user-images.githubusercontent.com/43855029/114417635-c39e1080-9b7f-11eb-8465-cbb9e0dff39e.png" alt="image" /></p>

<p>This procedure will adjust the <strong>β</strong> based on the training data. 
If there is any noise in training data, the model will not perform well for testing data. Thus, Regularization comes in and regularizes/shrinkage these 𝛽 towards zero.</p>

<p>There are 3 main types of Regularization.</p>
<ul>
  <li>Ridge Regression</li>
  <li>LASSO</li>
  <li>Elastics Nets</li>
</ul>

<h3 id="91-ridge-regression">9.1 Ridge Regression</h3>
<p><img src="https://user-images.githubusercontent.com/43855029/121278968-b496be80-c8a1-11eb-9117-250db80ca4d8.png" alt="image" /></p>

<p><strong>𝜆</strong>: Regularization Penalty, to be selected that the model minimized the error</p>

<p>The Ridge Regression loss function contains 2 elements: (1) RSS is actually the Ordinary Least Square (OLS) function for MLR and (2) The regularization term with <strong>𝜆</strong>:</p>

<p><img src="https://user-images.githubusercontent.com/43855029/121278778-4fdb6400-c8a1-11eb-9141-46d995d07061.png" alt="image" /></p>

<ul>
  <li>Selecting good <strong>𝜆</strong> is essential. In this case, Cross Validation method should be used</li>
  <li>Ridge Regression enforces <strong>β</strong> to be lower but not 0. By doing so, it will not get rid of irrelevant features but rather minimize their impact on the trained model.</li>
  <li>In statistics the coefficient esimated produced by this method is know as <strong>L2 norm</strong></li>
  <li>It is good practice to normalize predictors to the same scale before performing Ridge Regression (Because in OLS, the coefficients are scale equivalent)</li>
</ul>

<h4 id="911-implementation">9.1.1 Implementation</h4>
<p>Setting up training/testing model using the Stanford’s <a href="https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data">prostate cancer data</a></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">data</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/vuminhtue/Machine-Learning-Python/master/data/prostate_data.csv"</span><span class="p">)</span>
<span class="n">ind_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">"train"</span><span class="p">]</span><span class="o">==</span><span class="s">"T"</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">"train"</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">"lpsa"</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="n">ind_train</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">"lpsa"</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="o">~</span><span class="n">ind_train</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">"lpsa"</span><span class="p">][</span><span class="n">ind_train</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">"lpsa"</span><span class="p">][</span><span class="o">~</span><span class="n">ind_train</span><span class="p">]</span>
</code></pre></div></div>

<p>Predict using Ridge Regression method and Cross Validation approach:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span> <span class="k">as</span> <span class="n">mse</span>

<span class="n">n_lambda</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">lambdas</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span> <span class="n">n_lambda</span><span class="p">)</span>

<span class="n">MSE_train</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">MSE_test</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">ld</span> <span class="ow">in</span> <span class="n">lambdas</span><span class="p">:</span>
    <span class="n">ridgecv</span> <span class="o">=</span> <span class="n">RidgeCV</span><span class="p">(</span><span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="n">ld</span><span class="p">],</span> <span class="n">normalize</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
    <span class="n">model_RR</span> <span class="o">=</span> <span class="n">ridgecv</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_predRR_cv_train</span> <span class="o">=</span> <span class="n">model_RR</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">y_predRR_cv_test</span>  <span class="o">=</span> <span class="n">model_RR</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>    
    <span class="n">MSE_train</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_predRR_cv_train</span><span class="p">))</span>
    <span class="n">MSE_test</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_predRR_cv_test</span><span class="p">))</span>    
    <span class="n">coefs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_RR</span><span class="p">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="n">coef_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">coefs</span><span class="p">)</span>
<span class="n">coef_df</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">columns</span>
</code></pre></div></div>

<p>Plotting the Mean Square Error for Training and Testing dataset based on <strong>𝜆</strong> variation</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">lambdas</span><span class="p">),</span> <span class="n">MSE_train</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Training Set"</span><span class="p">)</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">lambdas</span><span class="p">),</span> <span class="n">MSE_test</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Testing Set"</span><span class="p">)</span>

<span class="n">ax1</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"log($</span><span class="se">\\</span><span class="s">lambda$)"</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"log($</span><span class="se">\\</span><span class="s">lambda$)"</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'MSE'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'MSE'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://user-images.githubusercontent.com/43855029/115435103-7bae6780-a1d7-11eb-995b-31a69408469e.png" alt="image" /></p>

<p>Plotting the coefficient of different predictors based on <strong>𝜆</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gca</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">coef_df</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">size</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">lambdas</span><span class="p">),</span> <span class="n">coef_df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>
    
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">coef_df</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="c1">#ax.set_xscale('log')
</span><span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"log($</span><span class="se">\\</span><span class="s">lambda$)"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Coefficients'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Ridge Regression Coefficients'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'tight'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/43855029/121300671-259b9d80-c8c5-11eb-8d65-8abf3ba5632f.png" alt="image" /></p>

<p>The plot shows different coefficients for all predictors with <strong>𝜆</strong> variation.</p>

<ul>
  <li>Ridge Regression’s pros: the pros of RR method over OLS is rooted in the bias variance trade-off. As when <strong>𝜆</strong> increases, the flexibility of RR fit decreases, hence decrease the variance but increase the bias</li>
  <li>Ridge Regression’s cons: <strong>β</strong> never be 0, so all predictors are included in the final model. Therefore, it is not good for best feature selection.</li>
</ul>

<h3 id="92-lasso-least-absolute-shrinkage--selection-operator">9.2 LASSO: Least Absolute Shrinkage &amp; Selection Operator</h3>
<p><img src="https://user-images.githubusercontent.com/43855029/121297875-f5ea9680-c8c0-11eb-96c7-b52291a7adbc.png" alt="image" /></p>

<ul>
  <li>In order to overcome the cons issue in Ridge Regression, the LASSO is introduced with the similar shrinkage parameter, but the different is not in square term of the coefficient but only absolute value</li>
  <li>Similar to Ridge Regression, LASSO also shrink the coefficient, but <strong>force</strong> coefficients to be equal to 0. Making it ability to perform <strong>feature selection</strong></li>
  <li>In statistics the coefficient esimated produced by this method is know as <strong>L1 norm</strong></li>
</ul>

<h4 id="921-implementation">9.2.1 Implementation</h4>
<p>Predict using Lasso method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="n">n_lambda</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">lambdas1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_lambda</span><span class="p">)</span>

<span class="n">MSE_train</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">MSE_test</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">ld</span> <span class="ow">in</span> <span class="n">lambdas1</span><span class="p">:</span>
    <span class="n">lassocv</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">ld</span><span class="p">)</span>
    <span class="n">model_LS</span> <span class="o">=</span> <span class="n">lassocv</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_predLS_cv_train</span> <span class="o">=</span> <span class="n">model_LS</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">y_predLS_cv_test</span> <span class="o">=</span> <span class="n">model_LS</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">MSE_train</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_predLS_cv_train</span><span class="p">))</span>
    <span class="n">MSE_test</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_predLS_cv_test</span><span class="p">))</span>
    <span class="n">coefs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_LS</span><span class="p">.</span><span class="n">coef_</span><span class="p">)</span>    
</code></pre></div></div>
<p>Plotting the Mean Square Error for Training and Testing dataset based on <strong>𝜆</strong> variation</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>


<span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">lambdas1</span><span class="p">),</span> <span class="n">MSE_train</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Training Set"</span><span class="p">)</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">lambdas1</span><span class="p">),</span> <span class="n">MSE_test</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Testing Set"</span><span class="p">)</span>

<span class="n">ax1</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"log($</span><span class="se">\\</span><span class="s">lambda$)"</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"log($</span><span class="se">\\</span><span class="s">lambda$)"</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'MSE'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'MSE'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://user-images.githubusercontent.com/43855029/115447429-80c6e300-a1e6-11eb-8808-d44145b77e5f.png" alt="image" /></p>

<p>Plotting the coefficient of different predictors based on <strong>𝜆</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coef_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">coefs</span><span class="p">)</span>
<span class="n">coef_df</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">columns</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gca</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">coef_df</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">size</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">lambdas1</span><span class="p">),</span> <span class="n">coef_df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>
    
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">coef_df</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span><span class="n">bbox_to_anchor</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">))</span>
<span class="c1">#ax.set_xscale('log')
</span><span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"log($</span><span class="se">\\</span><span class="s">lambda$)"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Coefficients'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'LASSO Coefficients'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'tight'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/43855029/121300854-5da2e080-c8c5-11eb-9eec-00c42b39dcbf.png" alt="image" /></p>

<p>The plot shows different coefficients for all predictors with <strong>𝜆</strong> variation. Depending on <strong>𝜆</strong> values that the <strong>β</strong> varying and it can be 0 at certain point.</p>

<h3 id="93-elastic-nets">9.3 Elastic Nets</h3>
<p>Elastic Nets Regularization is a method that includes both LASSO and Ridge Regression. Its formulation for the loss function is as following:
<img src="https://user-images.githubusercontent.com/43855029/114456877-615b0500-9bab-11eb-9298-028fcffc03ab.png" alt="image" /></p>

<ul>
  <li>𝛼=0: pure Ridge Regression</li>
  <li>𝛼=1: pure LASSO</li>
  <li>0&lt;𝛼&lt;1: Elastic Nets</li>
</ul>

<h4 id="931-implementation">9.3.1 Implementation</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>

<span class="n">MSE_train</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">MSE_test</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">ld</span> <span class="ow">in</span> <span class="n">lambdas1</span><span class="p">:</span>
    <span class="n">Elastic_cv</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">ld</span><span class="p">,</span><span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">model_EN</span> <span class="o">=</span> <span class="n">Elastic_cv</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_predEN_cv_train</span> <span class="o">=</span> <span class="n">model_EN</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">y_predEN_cv_test</span> <span class="o">=</span> <span class="n">model_EN</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">MSE_train</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_predEN_cv_train</span><span class="p">))</span>
    <span class="n">MSE_test</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_predEN_cv_test</span><span class="p">))</span>
    <span class="n">coefs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_EN</span><span class="p">.</span><span class="n">coef_</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>The ElasticNet mixing parameter, with <strong>0 &lt;= l1_ratio &lt;= 1.</strong></li>
  <li>For <strong>l1_ratio = 0</strong> the penalty is an L2 penalty (<strong>Ridge Regression</strong>).</li>
  <li>For <strong>l1_ratio = 1</strong> it is an L1 penalty (<strong>LASSO</strong>).</li>
  <li>For <strong>0 &lt; l1_ratio &lt; 1</strong>, the penalty is a combination of L1 and L2.</li>
</ul>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Regularization, Ridge Regression, LASSO, Elastic Nets</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="dimension-reduction" class="maintitle">Dimension Reduction</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 20 min
      <br />
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What happen when there are lots of covariates?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Learn how to apply PCA in ML model</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h2 id="10-principal-component-analysis">10 Principal Component Analysis</h2>
<ul>
  <li>Handy with large data</li>
  <li>Where many variables correlate with one another, they will all contribute strongly to the same principal component</li>
  <li>Each principal component sums up a certain percentage of the total variation in the dataset</li>
  <li>More Principal Components, more summarization of the original data sets</li>
</ul>

<h3 id="101-pca-formulation">10.1 PCA formulation</h3>
<ul>
  <li>For example, we have 3 data sets: <code class="language-plaintext highlighter-rouge">X, Y, Z</code></li>
  <li>We need to compute the covariance matrix <strong>M</strong> for the 3 data set:
<img src="https://user-images.githubusercontent.com/43855029/114459677-d67c0980-9bae-11eb-85b2-758a98f0cd29.png" alt="image" /></li>
</ul>

<p>in which, the covariance value between 2 data sets can be computed as:
<img src="https://user-images.githubusercontent.com/43855029/114459740-ea277000-9bae-11eb-9259-8ef1b233c0fa.png" alt="image" /></p>

<ul>
  <li>For the Covariance matrix <strong>M</strong>, we will find <strong>m</strong> eigenvectors and <strong>m</strong> eigenvalues</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Given mxm matrix, we can find m eigenvectors and m eigenvalues
- Eigenvectors can only be found for square matrix.
- Not every square matrix has eigenvectors
- A square matrix A and its transpose have the same eigenvalues but different eigenvectors
- The eigenvalues of a diagonal or triangular matrix are its diagonal elements.
- Eigenvectors of a matrix A with distinct eigenvalues are linearly independent.
</code></pre></div></div>

<p><strong>Eigenvector with the largest eigenvalue forms the first principal component of the data set
… and so on …</strong>*</p>

<h3 id="102-implementation">10.2 Implementation</h3>
<p>Here we gonna use iris data set:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">y</span><span class="p">[</span><span class="s">'Species'</span><span class="p">]</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">Categorical</span><span class="p">.</span><span class="n">from_codes</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">iris</span><span class="p">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="1021-compute-pca-using-sklearn">10.2.1 Compute PCA using sklearn:</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">PCs</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">)</span>
<span class="n">PCs</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">PCs</span><span class="p">,</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'PC1'</span><span class="p">,</span><span class="s">'PC2'</span><span class="p">,</span><span class="s">'PC3'</span><span class="p">,</span><span class="s">'PC4'</span><span class="p">])</span>
</code></pre></div></div>
<p>We can see that PCs computed from sklearn package are similar to newpca computed from using eigen vectors</p>
<h4 id="1022-explained-variance">10.2.2 Explained Variance</h4>
<p>The explained variance tells you how much information (variance) can be attributed to each of the principal components.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span>
</code></pre></div></div>
<p>In this example: the PC1(0.74) and PC2 (0.21) consume 0.95 percent of explained variance. Therefore, using 2 Principal Components should be good enough</p>
<h4 id="1023-application-of-pca-model-in-machine-learning">10.2.3 Application of PCA model in Machine Learning:</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span> <span class="k">as</span> <span class="n">acc_score</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1">#We choose number of principal components to be 2
</span>
<span class="n">X_train_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">)</span>
<span class="n">X_test_pca</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">))</span>
<span class="n">X_test_pca</span><span class="p">.</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'PC1'</span><span class="p">,</span><span class="s">'PC2'</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>

<span class="c1"># Use random forest to train model
</span><span class="n">model_RF</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span><span class="n">criterion</span><span class="o">=</span><span class="s">"gini"</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_pca</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[</span><span class="s">'Species'</span><span class="p">])</span>
<span class="n">y_pred_RF</span> <span class="o">=</span> <span class="n">model_RF</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">)</span>
<span class="n">acc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">[</span><span class="s">'Species'</span><span class="p">],</span><span class="n">y_pred_RF</span><span class="p">)</span>
</code></pre></div></div>
<p>Plotting the testing result with indicator of Wrong prediction</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gca</span><span class="p">()</span>

<span class="n">targets</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_pred_RF</span><span class="p">)</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s">'r'</span><span class="p">,</span> <span class="s">'g'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">]</span>

<span class="k">for</span> <span class="n">target</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span><span class="n">colors</span><span class="p">):</span>
    <span class="n">indp</span> <span class="o">=</span> <span class="n">y_pred_RF</span> <span class="o">==</span> <span class="n">target</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">indp</span><span class="p">,</span> <span class="s">'PC1'</span><span class="p">],</span> <span class="n">X_test_pca</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">indp</span><span class="p">,</span> <span class="s">'PC2'</span><span class="p">],</span><span class="n">c</span> <span class="o">=</span> <span class="n">color</span><span class="p">)</span>

<span class="c1"># Ploting the Wrong Prediction
</span><span class="n">ind</span> <span class="o">=</span> <span class="n">y_pred_RF</span><span class="o">!=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_test</span><span class="p">[</span><span class="s">'Species'</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span> <span class="s">'PC1'</span><span class="p">],</span><span class="n">X_test_pca</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span> <span class="s">'PC2'</span><span class="p">],</span><span class="n">c</span> <span class="o">=</span> <span class="s">'black'</span><span class="p">)</span>

<span class="c1">#axis control
</span><span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'setosa'</span><span class="p">,</span><span class="s">'versicolor'</span><span class="p">,</span><span class="s">'virginica'</span><span class="p">,</span><span class="s">'Wrong Prediction'</span><span class="p">])</span>  
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Testing set from Random Forest using PCA 2 components"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'PC1'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'PC2'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://user-images.githubusercontent.com/43855029/115561017-24fe6780-a283-11eb-8d9e-4a04b3a2e9a2.png" alt="image" /></p>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>PCA</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="neural-network" class="maintitle">Neural Network</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 20 min
      <br />
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>How to use Neural Network in Machine Learning model</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Learn how to use ANN in ML model</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h2 id="11-neural-network">11. Neural Network</h2>

<ul>
  <li>
    <p>Neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.</p>
  </li>
  <li>Neuron is a basic unit in a nervous system and is the most important component of the brain.</li>
  <li>In each Neuron, there is a cell body (node), dendrite (input signal) and axon (output signal to other neuron).</li>
  <li>If a Neuron received enough signal, it is then activated to decide whether or not it should transmitt the signal to other neuron or not.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114472746-da188c00-9bc0-11eb-913c-9dcd14f872ac.png" alt="image" />
<img src="https://user-images.githubusercontent.com/43855029/114472756-dd137c80-9bc0-11eb-863d-7c4d054efa89.png" alt="image" /></p>

<ul>
  <li>Formulation of Neural Network</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114472776-e997d500-9bc0-11eb-9f70-450389c912df.png" alt="image" /></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Here, x1,x2....xn are input variables. w1,w2....wn are weights of respective inputs.
b is the bias, which is summed with the weighted inputs to form the net inputs. 
Bias and weights are both adjustable parameters of the neuron.
Parameters are adjusted using some learning rules. 
The output of a neuron can range from -inf to +inf.
The neuron doesn’t know the boundary. So we need a mapping mechanism between the input and output of the neuron. 
This mechanism of mapping inputs to output is known as Activation Function.
</code></pre></div></div>
<ul>
  <li>Activation functions:
<img src="https://user-images.githubusercontent.com/43855029/114575672-6752f380-9c48-11eb-8d53-c78d052cdf17.png" alt="image" /></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="nb">xrange</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">xrange</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="nb">xrange</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'ReLU'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">xrange</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="nb">xrange</span><span class="p">),</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'Hyperbolic Tangent'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">xrange</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="nb">xrange</span><span class="p">)),</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'Sigmoid/Logistic'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">xrange</span><span class="p">,</span> <span class="nb">xrange</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'Linear'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">xrange</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">heaviside</span><span class="p">(</span><span class="nb">xrange</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'Step'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Neural network activation functions'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Input value (x)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Activation function output'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://user-images.githubusercontent.com/43855029/115588329-9e568400-a29c-11eb-92b3-abe0c2db16c5.png" alt="image" /></p>

<ul>
  <li>Neural Network formulation: Multi-Layer Perceptron (MLP)
<strong>Multi-layer Perceptron (MLP)</strong> is a supervised learning algorithm.
Given a set of features <code class="language-plaintext highlighter-rouge">X = x1, x2, ... xm</code>, and target <code class="language-plaintext highlighter-rouge">y</code>, MLP can learn a non-linear function approximator for either classification or regression.</li>
</ul>

<p>Between the input and the output layer, there can be one or more non-linear layers, called hidden layers. Figure below shows a one hidden layer MLP with scalar output.</p>

<p><img src="https://user-images.githubusercontent.com/43855029/114472972-51e6b680-9bc1-11eb-9e78-90ec739844ee.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/43855029/114575549-48546180-9c48-11eb-8c9c-c5eac3180df1.png" alt="image" /></p>

<p><strong>The advantages of Multi-layer Perceptron:</strong></p>
<ul>
  <li>Capability to learn non-linear models.</li>
  <li>Capability to learn models in real-time (on-line learning) using partial_fit.</li>
</ul>

<p><strong>The disadvantages of Multi-layer Perceptron:</strong></p>
<ul>
  <li>MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.</li>
  <li>MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.</li>
  <li>MLP is sensitive to feature scaling.</li>
</ul>

<h3 id="111-type-of-neural-network-multi-layer-perceptron-in-sklearn">11.1. Type of Neural Network Multi-Layer Perceptron in sklearn</h3>
<p>There are 2 main types of MLP in sklearn, depending on the model output:</p>
<ul>
  <li>MLPClassifier: for Classification problem</li>
  <li>MLPRegressor: for Regression problem</li>
</ul>

<h3 id="112-implementation-with-classification-problem">11.2. Implementation with Classification problem</h3>
<p>Here we use <strong>iris</strong> data for Classification problem</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">y</span><span class="p">[</span><span class="s">'Species'</span><span class="p">]</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">Categorical</span><span class="p">.</span><span class="n">from_codes</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">iris</span><span class="p">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<p>The Class <strong>MLPClassifier</strong> implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation.
There are lots of parameters in MLPClassifier:</p>
<ul>
  <li><strong>hidden_layer_sizes</strong> which is the number of hidden layers and neurons for each layer. Default=<code class="language-plaintext highlighter-rouge">(100,)</code>
for example <code class="language-plaintext highlighter-rouge">hidden_layer_sizes=(100,)</code> means there is 1 hidden layers used, with 100 neurons.
for example <code class="language-plaintext highlighter-rouge">hidden_layer_sizes=(50,20)</code> means there are 2 hidden layers used, the first layer has 50 neuron and the second has 20 neurons.</li>
  <li><strong>solver</strong> <code class="language-plaintext highlighter-rouge">lbfgs, sgd, adam</code>. Default=<code class="language-plaintext highlighter-rouge">adam</code></li>
  <li><strong>activation</strong> <code class="language-plaintext highlighter-rouge">identity, logistic, tanh, relu</code>. Default=’relu`</li>
</ul>

<p>More information can be found <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html">here</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>
<span class="n">model_NN</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span><span class="n">solver</span><span class="o">=</span><span class="s">'lbfgs'</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[</span><span class="s">'Species'</span><span class="p">])</span>
<span class="n">model_NN</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span><span class="n">y_test</span><span class="p">[</span><span class="s">'Species'</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="113-implementation-with-regression-problem">11.3. Implementation with Regression problem</h3>
<ul>
  <li>Class <strong>MLPRegressor</strong> implements a multi-layer perceptron (MLP) that trains using backpropagation with no activation function in the output layer, which can also be seen as using the identity function as activation function.</li>
  <li>Therefore, it uses the square error as the loss function, and the output is a set of continuous values.</li>
</ul>

<p>Here we use <strong>airquality</strong> data from Regression espisode:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">KNNImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">data_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/vuminhtue/Machine-Learning-Python/master/data/r_airquality.csv'</span><span class="p">))</span>

<span class="n">imputer</span> <span class="o">=</span> <span class="n">KNNImputer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s">"uniform"</span><span class="p">)</span>
<span class="n">data_knnimpute</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">imputer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data_df</span><span class="p">))</span>
<span class="n">data_knnimpute</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">data_df</span><span class="p">.</span><span class="n">columns</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data_knnimpute</span><span class="p">[[</span><span class="s">'Temp'</span><span class="p">,</span><span class="s">'Wind'</span><span class="p">,</span><span class="s">'Solar.R'</span><span class="p">]],</span>
                                                    <span class="n">data_knnimpute</span><span class="p">[</span><span class="s">'Ozone'</span><span class="p">],</span>
                                                    <span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</code></pre></div></div>
<p>Fit <strong>MLPRegressor</strong> model</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPRegressor</span>
<span class="n">model_NN</span> <span class="o">=</span> <span class="n">MLPRegressor</span><span class="p">(</span><span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span><span class="n">solver</span><span class="o">=</span><span class="s">'lbfgs'</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">model_NN</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="114-tips-on-using-mlp">11.4. Tips on using MLP</h3>
<ul>
  <li>Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data.</li>
  <li>There is built-in regularization parameter <strong>alpha</strong> (<code class="language-plaintext highlighter-rouge">Default 0.0001</code>). To find reasonable <strong>alpha</strong>, it’s best to use <strong>GridSearchCV</strong>, usually in the range 10.0**-np.arange(1, 7)</li>
  <li>Empirically, we observed that <strong>L-BFGS</strong> converges faster and with better solutions on small datasets. For relatively large datasets, however, <strong>Adam</strong> is very robust. It usually converges quickly and gives pretty good performance. <strong>SGD</strong> with momentum or nesterov’s momentum, on the other hand, can perform better than those two algorithms if learning rate is correctly tuned.</li>
  <li>Since backpropagation has a high time complexity, it is advisable to start with smaller number of hidden neurons and few hidden layers for training.</li>
  <li>The loss function for Classifier is <strong>Cross-Entropy</strong> while for Regression is <strong>Square-Error</strong></li>
</ul>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>ANN</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="support-vector-machine" class="maintitle">Support Vector Machine</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 20 min
      <br />
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>How to use Support Vector Machine in Machine Learning model</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Learn how to use SVM in ML model</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h2 id="12-support-vector-machine">12. Support Vector Machine</h2>
<ul>
  <li>A support vector machine is a very important and versatile machine learning algorithm,</li>
  <li>It is capable of doing linear and nonlinear classification, regression and outlier detection.</li>
  <li>It is preferred over other classification algorithms because it uses less computation and gives notable accuracy.</li>
  <li>It is good because it gives reliable results even if there is less data</li>
  <li>The objective of the support vector machine (SVM) algorithm is to find a hyperplane in an N-dimensional space that distinctly classifies the data points.</li>
</ul>

<h3 id="121-applications-of-support-vector-machine">12.1. Applications of Support Vector Machine:</h3>
<p><img src="https://user-images.githubusercontent.com/43855029/114576381-1394da00-9c49-11eb-95b1-cff9d87c6029.png" alt="image" /></p>

<h3 id="122-explanation">12.2. Explanation</h3>
<ul>
  <li>To separate the two classes of data points, there are many possible hyperplanes that could be chosen</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114577032-af264a80-9c49-11eb-8e6c-b45120743f0d.png" alt="image" /></p>

<ul>
  <li>SVM’s objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes.
Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114576981-a2a1f200-9c49-11eb-9921-b0bff879c97e.png" alt="image" /></p>

<ul>
  <li>Example of hyperplane in 2D and 3D position:</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114577340-eac11480-9c49-11eb-8ff9-4aa3e61b1c86.png" alt="image" /></p>

<ul>
  <li>Support vectors (<strong>SVs</strong>) are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane.
Using <strong>SVs</strong> to maximize the margin of the classifier.
Removing <strong>SVs</strong> will change the position of the hyperplane. These are the points that help us build our SVM.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114577489-09271000-9c4a-11eb-8b4a-b7837463288f.png" alt="image" /></p>

<h3 id="123-svms-kernel">12.3. SVM’s kernel</h3>
<h4 id="1231--for-linear-separable-data-it-is-quite-straight-forward-to-create-a-hyperplane-to-distinguish-them">12.3.1.  For linear separable data, it is quite straight forward to create a hyperplane to distinguish them</h4>
<p><img src="https://user-images.githubusercontent.com/43855029/115606536-d0beac00-a2b1-11eb-9ba7-18dbc1c7ff28.png" alt="image" /></p>

<h4 id="1232-for-linearly-non-separable-data-svm-makes-use-of-kernel-tricks-to-make-it-linearly-separable">12.3.2. For linearly non-separable data, SVM makes use of kernel tricks to make it linearly separable.</h4>
<p><img src="https://user-images.githubusercontent.com/43855029/115606589-e3d17c00-a2b1-11eb-98a2-aebd6417eaf6.png" alt="image" /></p>

<ul>
  <li>The concept of transformation of non-linearly separable data into linearly separable is called Cover’s theorem - “given a set of training data that is not linearly separable, with high probability it can be transformed into a linearly separable training set by projecting it into a higher-dimensional space via some non-linear transformation”.</li>
  <li>Kernel tricks help in projecting data points to the higher dimensional space by which they became relatively more easily separable in higher-dimensional space.</li>
  <li>Kernel tricks also known as Generalized dot product.</li>
  <li>Kernel tricks are the way of calculating dot product of two vectors to check how much they make an effect on each other.</li>
  <li>According to Cover’s theorem the chances of linearly non-separable data sets becoming linearly separable increase in higher dimensions.</li>
  <li>Kernel functions are used to get the dot products to solve SVM constrained optimization.</li>
</ul>

<p>The following <a href="https://gist.github.com/WittmannF/60680723ed8dd0cb993051a7448f7805">kernel trick</a> compared different kernel <code class="language-plaintext highlighter-rouge">‘linear’ , ’poly’ , ‘rbf’ , ‘sigmoid’</code>:</p>

<p><img src="https://user-images.githubusercontent.com/43855029/115606803-2d21cb80-a2b2-11eb-9421-64642195fa5a.png" alt="image" /></p>

<h3 id="124-implementation">12.4. Implementation</h3>
<p>Here we use the regular <strong>iris</strong> dataset with Classification problem</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">target</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</code></pre></div></div>

<p>Fit Support Vector Classifier model</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">mlxtend.plotting</span> <span class="kn">import</span> <span class="n">plot_decision_regions</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Linear SVM"</span><span class="p">,</span> <span class="s">"RBF SVM"</span><span class="p">,</span> <span class="s">"Poly SVM"</span><span class="p">,</span> <span class="s">"Sigmoid SVM"</span><span class="p">]</span>
<span class="n">classifiers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">"linear"</span><span class="p">),</span>
    <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">"rbf"</span><span class="p">),</span>
    <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">"poly"</span><span class="p">),</span>
    <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">"sigmoid"</span><span class="p">)]</span>

<span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">27</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">jet</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">clf</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">classifiers</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> 
                      <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
                      <span class="n">clf</span><span class="o">=</span><span class="n">clf</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">handles</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="p">,</span><span class="n">iris</span><span class="p">.</span><span class="n">target_names</span><span class="p">)</span>
    <span class="n">i</span><span class="o">+=</span><span class="mi">1</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/43855029/121306330-b3c75200-c8cc-11eb-996d-5a98e7619f98.png" alt="image" /></p>

<p>In this model, <strong>C</strong> is the regularization parameter <code class="language-plaintext highlighter-rouge">Default C=1</code>. The strength of the regularization is inversely proportional to C. Must be strictly positive.</p>

<h3 id="125-tips-on-using-svm">12.5. Tips on using SVM</h3>
<ul>
  <li>Setting <code class="language-plaintext highlighter-rouge">C=1</code> is reasonable choice for default. If you have a lot of noisy observations you should decrease it: decreasing C corresponds to more regularization.</li>
  <li>More information <a href="https://scikit-learn.org/stable/modules/svm.html#tips-on-practical-use">here</a></li>
</ul>

<h3 id="126-pros-of-svm">12.6. Pros of SVM</h3>
<ul>
  <li>High stability due to dependency on support vectors and not the data points.</li>
  <li>Does not get influenced by Outliers.</li>
  <li>No assumptions made of the datasets.</li>
  <li>Numeric predictions problem can be dealt with SVM.</li>
</ul>

<h3 id="127-cons-of-svm">12.7. Cons of SVM</h3>
<ul>
  <li>Blackbox method.</li>
  <li>Inclined to overfitting method.</li>
  <li>Very rigorous computation.</li>
</ul>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>SVM</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="k-nearest-neighbour" class="maintitle">K-Nearest Neighbour</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 20 min
      <br />
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>How to use K-Nearest Neighbour in Machine Learning model</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Learn how to use KNN in ML model</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h2 id="13-k-nearest-neighbour">13. K-Nearest Neighbour</h2>
<ul>
  <li>Simplicity but powerful and fast for certain task</li>
  <li>Work for both classification and regression</li>
  <li>Named as Instance Based Learning; Non-parametrics; Lazy learner</li>
  <li>Work well with small number of inputs</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114582045-3d043480-9c4e-11eb-8698-e1c31840401a.png" alt="image" /></p>

<h3 id="131-explanation">13.1. Explanation</h3>

<p><img src="https://user-images.githubusercontent.com/43855029/114582162-573e1280-9c4e-11eb-8a17-e0d91a38452e.png" alt="image" /></p>

<ul>
  <li>In KNN, the most important parameter is the K group and the distance computed between points.</li>
  <li>Euclide distance:</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114582319-7a68c200-9c4e-11eb-93f2-37324c034784.png" alt="image" /></p>

<h3 id="132-implementation">13.2. Implementation</h3>
<p>Here we gonna use the <strong>iris</strong> dataset again:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">target</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</code></pre></div></div>

<p>Train the model using KNN model with 3 nearest neighbors</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="n">model_KNN</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">model_KNN</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">model_KNN</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="https://user-images.githubusercontent.com/43855029/114583370-86a14f00-9c4f-11eb-96a0-59b3c5376952.png" alt="image" /></p>

<h3 id="133-other-similar-models-from-sklearnneighbors">13.3. Other similar models from sklearn.neighbors:</h3>
<ul>
  <li>KNeighborsRegressor: KNN estimators for Regression problem with continuous data</li>
  <li>NearestCentroid</li>
  <li>and <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors">more</a></li>
</ul>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>KNN</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="unsupervised-learning" class="maintitle">Unsupervised Learning</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 20 min
      <br />
      <strong>Exercises:</strong> 0 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What is Unsupervised Learning in Machine Learning model</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Learn how to use K-mean clustering in ML model</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h1 id="14-unsupervised-learning">14. Unsupervised Learning</h1>

<ul>
  <li>No labels are given to the learning algorithm leaving it on its own to find structure in its input.</li>
  <li>Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114584282-82c1fc80-9c50-11eb-9342-41e5592e7b67.png" alt="image" /> <img src="https://user-images.githubusercontent.com/43855029/114584314-89507400-9c50-11eb-9c54-5a589075fd48.png" alt="image" /></p>

<ul>
  <li>Used when no feature output data</li>
  <li>Often used for clustering data</li>
  <li>Typical method:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>K-means clustering
Hierarchical clustering
Ward clustering
Partition Around Median (PAM)
</code></pre></div>    </div>
    <h2 id="141-k-means-clustering">14.1. K-means clustering</h2>
    <h3 id="1411-explanation-of-k-means-clustering-method">14.1.1. Explanation of K-means clustering method:</h3>
  </li>
  <li>Given a set of data, we choose K=2 clusters to be splited:</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114584415-a5ecac00-9c50-11eb-8919-807f83ddf23a.png" alt="image" /></p>

<ul>
  <li>First select 2 random centroids (denoted as red and blue X)</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114584573-d16f9680-9c50-11eb-9dc4-8d918919f565.png" alt="image" /></p>

<ul>
  <li>Compute the distance between 2 centroid red X and blue X with all the points (for instance using Euclidean distance) and compare with each other. 2 groups are created with shorter distance to 2 centroids</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114584860-0bd93380-9c51-11eb-9afc-3bb9510e9c34.png" alt="image" /></p>

<ul>
  <li>Now recompute the <strong>new</strong> centroids of the 2 groups (using mean value of all points in the same groups):</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114585002-34f9c400-9c51-11eb-83e0-b5769abf6cd3.png" alt="image" /></p>

<ul>
  <li>Compute the distance between 2 <strong>new</strong> centroids and all the points. We have 2 new groups:</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114585030-3b883b80-9c51-11eb-8f69-29f6e406e215.png" alt="image" /></p>

<ul>
  <li>Repeat the last 2 steps until <strong>no more new centroids</strong> created. The model reach equilibrium:</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114585223-6b374380-9c51-11eb-8663-27474956ec61.png" alt="image" /></p>

<h3 id="1412-example-with-k3">14.1.2. Example with K=3</h3>
<p><img src="https://user-images.githubusercontent.com/43855029/114585361-8e61f300-9c51-11eb-965e-dc4d57e9c0eb.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/43855029/114585502-b81b1a00-9c51-11eb-8015-973216b450ce.png" alt="image" /></p>

<h3 id="1413-implementation">14.1.3. Implementation</h3>
<p>Here we use the iris data set with only predictors</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">.</span><span class="n">data</span>
</code></pre></div></div>

<p>Apply Kmeans and plotting</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">model_KMeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">model_KMeans</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">3</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">model_KMeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'KMeans clustering with 3 clusters'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/43855029/115735833-c99ea900-a358-11eb-87d8-774efc7fa459.png" alt="image" /></p>

<h3 id="1414-how-to-find-optimal-k-values">14.1.4. How to find optimal K values:</h3>
<h4 id="14141-elbow-approach">14.1.4.1. Elbow approach</h4>
<ul>
  <li>Similar to KNN method for supervised learning, for K-means approach, we are able to use Elbow approach to find the optimal K values.</li>
  <li>The Elbow approach ues the Within-Cluster Sum of Square (WSS) to measure the compactness of the clusters:
<img src="https://user-images.githubusercontent.com/43855029/114587068-4d6ade00-9c53-11eb-932d-0de0c9edef83.png" alt="image" /></li>
</ul>

<p>The optimal K-values can be found from the Elbow using <strong>method=”wss”</strong>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">wss</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">inertia_</span><span class="p">)</span>
    
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span><span class="n">wss</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span><span class="n">wss</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Number of Clusters k"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Within Sum of Square"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Optimal number of clusters based on WSS Method"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://user-images.githubusercontent.com/43855029/115737965-9b21cd80-a35a-11eb-9bcd-0d63e685ec0f.png" alt="image" /></p>

<h4 id="14142-gap-statistics-approach">14.1.4.2. Gap-Statistics approach</h4>
<ul>
  <li>Developed by Prof. Tibshirani et al in Stanford</li>
  <li>Applied to any clustering method (K-means, Hierarchical)</li>
  <li>Maximize the Gap function:</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/114586376-95d5cc00-9c52-11eb-9b71-ed330cfc50bc.png" alt="image" /></p>

<p>E*n: expectation under a sample size of n from the reference distribution
<img src="https://user-images.githubusercontent.com/43855029/114586396-9b331680-9c52-11eb-9b83-955aa256e623.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/43855029/114586456-af771380-9c52-11eb-9fdb-99cc8df854fb.png" alt="image" /></p>

<p><strong>Installation:</strong></p>

<p>This version of Gap Statistics is not official. Until the moment of writing this documentation, no official Gap Statistics has been released in Python.
We use the version from <a href="https://github.com/milesgranger/gap_statistic">milesgranger’s github</a></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pip</span> <span class="n">install</span> <span class="n">git</span><span class="o">+</span><span class="n">git</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">milesgranger</span><span class="o">/</span><span class="n">gap_statistic</span><span class="p">.</span><span class="n">git</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">gapstat</span><span class="o">-</span><span class="n">rs</span>
</code></pre></div></div>
<p>Implement Gap-Statistics:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">gap_statistic</span> <span class="kn">import</span> <span class="n">OptimalK</span>

<span class="n">optimalK</span> <span class="o">=</span> <span class="n">OptimalK</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># No parallel
</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="n">optimalK</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="mi">4</span><span class="p">],</span> <span class="n">cluster_array</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Optimal clusters: '</span><span class="p">,</span> <span class="n">n_clusters</span><span class="p">)</span>
</code></pre></div></div>

<p>Plot Gap-Statistics:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimalK</span><span class="p">.</span><span class="n">gap_df</span><span class="p">.</span><span class="n">n_clusters</span><span class="p">,</span> <span class="n">optimalK</span><span class="p">.</span><span class="n">gap_df</span><span class="p">.</span><span class="n">gap_value</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">optimalK</span><span class="p">.</span><span class="n">gap_df</span><span class="p">[</span><span class="n">optimalK</span><span class="p">.</span><span class="n">gap_df</span><span class="p">.</span><span class="n">n_clusters</span> <span class="o">==</span> <span class="n">n_clusters</span><span class="p">].</span><span class="n">n_clusters</span><span class="p">,</span>
            <span class="n">optimalK</span><span class="p">.</span><span class="n">gap_df</span><span class="p">[</span><span class="n">optimalK</span><span class="p">.</span><span class="n">gap_df</span><span class="p">.</span><span class="n">n_clusters</span> <span class="o">==</span> <span class="n">n_clusters</span><span class="p">].</span><span class="n">gap_value</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Cluster Count'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Gap Value'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Gap Values by Cluster Count'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://user-images.githubusercontent.com/43855029/115745658-a298a500-a361-11eb-8071-6af68f7eb428.png" alt="image" /></p>

<h2 id="142-comparison-between-different-clustering-methods-in-sklearn">14.2. Comparison between different clustering methods in sklearn:</h2>
<ul>
  <li>This is example from <a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html">sklearn</a></li>
  <li>The source code for image below can be found <a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-download-auto-examples-cluster-plot-cluster-comparison-py">here</a></li>
</ul>

<p><img src="https://user-images.githubusercontent.com/43855029/115748324-0f14a380-a364-11eb-8a06-6d073b4d99c4.png" alt="image" /></p>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>K-mean</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="mini-project" class="maintitle">Mini-Project</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 10 min
      <br />
      <strong>Exercises:</strong> 180 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What do you learn from Machine Learning workshop using scikit-learn?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Learn how to apply Machine Learning into your real project</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h1 id="15-mini-project">15. MINI PROJECT</h1>

<h2 id="description-of-mini-project">Description of Mini Project:</h2>

<ul>
  <li>This Mini Project evaluates the ability of you working on a sample Data Science project from scratch.</li>
  <li>The project is about Supervised Machine Learning only</li>
  <li>It requires you to download data, clean data, split into training/testing and apply any machine learning model and analyse the output.</li>
  <li>You can also use what you have learn during the Deep Learning workshop as well (Optional) like CNN, RNN or just traditional Fully connected Dense layer.</li>
</ul>

<h2 id="requirement">Requirement:</h2>
<ul>
  <li>You can write the script in Jupyter Notebook and save it as your_username.ipynb and send it to my email: <strong>tuev@clemson.edu</strong>. You can also upload to your github page and share to me</li>
  <li><strong>Note</strong>: in order to get the <strong>Certificate of Attendence</strong>, you should send me the ipynb script on or before <strong>10/31/2021</strong>.</li>
</ul>

<h2 id="data">Data</h2>
<ul>
  <li>You can use <em>any</em> data in your field of expertise (most preferred method). If so, please send along the dataset with the ipynb file</li>
  <li>You can also use <em>Kaggle</em> data which can be found <a href="https://www.kaggle.com/datasets">here</a> (second preferred method). Please elaborate in the ipynb how did you retrieve the data</li>
  <li>You can use use <em>Titanic</em> data from our <a href="https://github.com/clemsonciti/Workshop-Python-ML/tree/master/data/Titanic_data">repo</a>. 
Description for Titanic data can be found <a href="https://www.kaggle.com/c/titanic/data">here</a> (third preferred method)</li>
  <li>You can use any of the <em>Toy</em> dataset from scikit learn which can be found <a href="https://scikit-learn.org/stable/datasets/toy_dataset.html">here</a>, <strong>except</strong> the <em>iris</em> data.</li>
</ul>

<h2 id="method">Method</h2>
<p>In the ipynb solution file I would like to see the following:</p>
<ul>
  <li>Clearly state the objective of the mini-project on Supervised Machine Learning</li>
  <li>Brief explanation about the data that you will be using: source, predictors, predictand</li>
  <li>Type of ML model output: Continuous or Classification?</li>
  <li>Read in the data</li>
  <li>Clean &amp; Standardized the input data if needed</li>
  <li>Split data to training/testing (You can also use Cross Validation if needed, not required)</li>
  <li>You can use any Regularization (variable selection) or PCA if needed (not required)</li>
  <li>Construct Machine Learning model to training set and explain why do you want to use that algorithm (any model is fine for me)</li>
  <li>Apply Machine Learning model to predict the output from testing set</li>
  <li>Evaluate the output using any of the given method in chapter 4</li>
  <li>Confirm if your ML model is good or bad?</li>
</ul>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Self-project</p>
</li>
    
  </ul>
</blockquote>

<hr />


</article>


<!--  
      






<footer>
  <div class="row">
    <div class="col-md-6 copyright" align="left">
	
	Licensed under <a href="">CC-BY 4.0</a> 2018–2022
	by <a href="">The Carpentries</a>
        <br>
        Licensed under <a href="">CC-BY 4.0</a> 2016–2018
	by <a href="">Software Carpentry Foundation</a>
	
    </div>
  </div>
  <div class="row">
    <div class="col-md-12" align="center">
      Using <a href="https://github.com/carpentries/styles/">The Carpentries style</a>
      version <a href="https://github.com/carpentries/styles/releases/tag/v9.5.3">9.5.3</a>.
    </div>
  </div>
</footer>

       -->
    </div>
    
<script src="../assets/js/jquery.min.js"></script>
<script src="../assets/js/bootstrap.min.js"></script>
<script src="../assets/js/lesson.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-37305346-2', 'auto');
  ga('send', 'pageview');
</script>

  </body>
</html>

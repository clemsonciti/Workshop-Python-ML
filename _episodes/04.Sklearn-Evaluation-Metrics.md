---
title: "Evaluation Metrics with Scikit-Learn"
teaching: 20
exercises: 0
questions:
- "How do we measure the accuracy of ML model"
objectives:
- "Learn different metrics with sklearn"
keypoints:
- "sklearn, metrics"
---

# Evaluation Metrics

- Evaluation Metric is an essential part in any Machine Learning project.
- It measures how good or bad is your Machine Learning model
- Different Evaluation Metrics are used for Regression model (Continuous output) or Classification model (Categorical output).

## 1. Regression model Evaluation Metrics

### 1.1. Correlation Coefficient (R) or Coefficient of Determination (R2):

![image](https://user-images.githubusercontent.com/43855029/120684829-2b305800-c46d-11eb-804b-cb469dcf914b.png)

```python
from sklearn import metrics
metrics.r2_score(y_test,y_pred)
```

### 1.2. Root Mean Square Error (RMSE) or Mean Square Error (MSE)

![image](https://user-images.githubusercontent.com/43855029/120685261-a8f46380-c46d-11eb-9860-be03f054326a.png)

```python
from sklearn import metrics
metrics.mean_squared_error(y_test,y_pred,squared=False) # RMSE
metrics.mean_squared_error(y_test,y_pred,squared=True) # MSE
```

## 2. Classification model Evaluation Metrics

### 2.1. Confusion Matrix
- A confusion matrix is a technique for summarizing the performance of a classification algorithm.
- You can learn more about Confusion Matrix [here](https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/)

For binary output (classification problem with only 2 output type, also most popular):

![image](https://user-images.githubusercontent.com/43855029/120687356-efe35880-c46f-11eb-950f-5feef237a4c1.png)

### 2.2. Accuracy

The most common metric for classification is accuracy, which is the fraction of samples predicted correctly as shown below:

![image](https://user-images.githubusercontent.com/43855029/120695672-a7309d00-c479-11eb-8882-0898ac2c3ef1.png)

```python
from sklearn import metrics
metrics.accuracy_score(y_test,y_pred)
```

### 2.3. Precision 

Precision is the fraction of predicted positives events that are actually positive as shown below:

![image](https://user-images.githubusercontent.com/43855029/120697273-849f8380-c47b-11eb-92e7-24ed6a8f9c06.png)

### 2.4. Recall

Recall (also known as sensitivity) is the fraction of positives events that you predicted correctly as shown below:

![image](https://user-images.githubusercontent.com/43855029/120697357-9c770780-c47b-11eb-8386-5d9e51251041.png)

### 2.5. F1 score

The f1 score is the harmonic mean of recall and precision, with a higher score as a better model. The f1 score is calculated using the following formula:

![image](https://user-images.githubusercontent.com/43855029/120697414-af89d780-c47b-11eb-9c56-21d578104717.png)

```python
metrics.precision_recall_fscore_support(y_test,y_pred,average='binary')
```


